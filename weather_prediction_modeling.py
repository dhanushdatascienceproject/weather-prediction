# -*- coding: utf-8 -*-
"""Weather_Prediction_Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/weather-prediction-modeling-ipynb-738dfd4b-1be2-45d8-8386-3daae48f759b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240820/auto/storage/goog4_request%26X-Goog-Date%3D20240820T155243Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D47f797de8e029a9d3b8d76e08240dcbf8f80e9b2c07eb82cf35bd2d2dc9953e2f2e8f655d9a3ea69c935b6af7ed6d83f34f3797ba21cf8df853cf00cc5d81dafacacb92b931a1cc371c4c4fc0d542b59f717077ec82823e6f0c98177b246dbd81480357671dfff2d51c1b31dc2abaaf7515738c4e16864001f3bd1c6ae773de71766c2db0a992c93aa5f23671d5a64cc27bf5409156d48927503db48c91662b5c8a7e529e6774c1d17d61242dc5d948ef5fa8e0e91859bb61798543088bd5e7f21f7ef8b96a508b8ca881f40e73435fc9ac07715bb64a7bf4f354ccdea307e2b807b5e278ef1807ef8e010c8b1970acb651e575da8ec4cb6029977b27d5cf3cc

# Step 1: Import necessary libraries
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

"""# Step 2: Load the datasets

"""

weather_data = pd.read_csv('/content/weather_prediction_dataset.csv')
bbq_labels = pd.read_csv('/content/weather_prediction_bbq_labels.csv')

"""# # Step 3:show datasets"""

weather_data.head()

bbq_labels.head()

"""# I wanna select OSLO_BBQ_weather as y and related features as x

# Step 4: Merge datasets on the 'DATE' column
"""

merged_data = pd.merge(weather_data, bbq_labels, on='DATE')
merged_data .head()

"""# Step 5:Check for missing values in merged_data

"""

# Count missing values in all column using isna() method
missing_counts = merged_data.isna().sum().sum()
missing_counts

merged_data.head()

"""# Step 6: define X and y"""

# Filter columns related to Oslo and include the date
oslo_columns = merged_data.filter(like='OSLO')

# Create X with 'DATE', 'MONTH', and Oslo-related columns
X = pd.concat([merged_data[['DATE', 'MONTH']], merged_data[oslo_columns.columns]], axis=1)

# Drop the target variable OSLO_BBQ_weather from X
X.drop('OSLO_BBQ_weather', axis=1, inplace=True)

# Target variable
y = merged_data['OSLO_BBQ_weather']

X

merged_data[oslo_columns.columns]

y

# Convert boolean values to numeric (1 for True, 0 for False)
y = y.astype(int)
y

"""# Step 6: correlations"""

import numpy as np

# Calculate Pearson correlation coefficients
correlations = X.corrwith(y)

# Absolute values of correlations for better interpretation
correlations = correlations.abs()

# Sort correlations in descending order
sorted_correlations = correlations.sort_values(ascending=False)

print(sorted_correlations)

"""# Step 7: Feature Selection

### About Correlation

1-Strong Correlation (Close to Â±1):
When the absolute value of the correlation coefficient is close to 1 (either positive or negative), it indicates a strong linear relationship between the two variables. For example, an absolute correlation coefficient of 0.8 or higher suggests a strong linear association between the variables.

2-Weak Correlation (Close to 0):

When the absolute value of the correlation coefficient is close to 0, it indicates a weak linear relationship or no linear relationship between the variables. A coefficient near 0 means that changes in one variable are not strongly associated with changes in the other variable.
"""

import pandas as pd
from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif, RFE, SelectFromModel
from sklearn.linear_model import LogisticRegression  # Example classifier for RFE
from sklearn.ensemble import RandomForestClassifier  # Example classifier for SelectFromModel
from sklearn.model_selection import train_test_split

# Assuming X is your feature matrix and y is your target variable
# Replace these with your actual data

# Calculate correlation coefficients between features and target
correlations = X.corrwith(y)

# Select features with high correlation coefficients
high_corr_features = correlations[abs(correlations) > 0.3].index  # Adjust correlation threshold as needed

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the number of features to select for SelectKBest and RFE
k_best_features = 10  # Adjust as needed
rfe_num_features = 15  # Adjust as needed

# Step 1: Apply different feature selection methods
# Method 1: SelectKBest with mutual information
skb_mi = SelectKBest(score_func=mutual_info_classif, k=k_best_features)
X_skb_mi = skb_mi.fit_transform(X_train, y_train)
selected_features_skb_mi = X.columns[skb_mi.get_support(indices=True)]

# Method 2: SelectKBest with ANOVA F-value
skb_f = SelectKBest(score_func=f_classif, k=k_best_features)
X_skb_f = skb_f.fit_transform(X_train, y_train)
selected_features_skb_f = X.columns[skb_f.get_support(indices=True)]

# Method 3: Recursive Feature Elimination (RFE) with Logistic Regression
rfe_lr = RFE(LogisticRegression(), n_features_to_select=rfe_num_features)
X_rfe_lr = rfe_lr.fit_transform(X_train, y_train)
selected_features_rfe_lr = X.columns[rfe_lr.get_support(indices=True)]

# Method 4: Lasso-based feature selection
lasso_model = RandomForestClassifier()  # Example classifier for SelectFromModel
lasso_selector = SelectFromModel(lasso_model)
X_lasso = lasso_selector.fit_transform(X_train, y_train)
selected_features_lasso = X.columns[lasso_selector.get_support(indices=True)]

# Step 2: Intersect the selected features from different methods
selected_features_intersection = set(selected_features_skb_mi) & set(selected_features_skb_f) & \
                                set(selected_features_rfe_lr) & set(selected_features_lasso)

# Add high correlated features to the selected features
final_selected_features = list(selected_features_intersection.union(high_corr_features))

print("Final Selected Features:", final_selected_features)

high_corr_features

selected_features_skb_mi

selected_features_skb_f

selected_features_rfe_lr

selected_features_lasso

selected_features_intersection

final_selected_features

"""# Step 8: Define new X and y based on new features"""

X=X[final_selected_features]
X

y

"""### Dropout is a regularization technique commonly used in neural networks to prevent overfitting. While dropout can help in improving the generalization ability of the model and reduce overfitting, it is not specifically designed to address multicollinearity issues in the input features.

# Step 9: Train and evaluate models
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import warnings

# Ignore warnings
warnings.filterwarnings("ignore")

# Assuming X and y are already defined and preprocessed

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define a dictionary of models
models = {
    'Logistic Regression': LogisticRegression(),
    'Support Vector Machine': SVC(),
    'Random Forest': RandomForestClassifier(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Gaussian Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier(),
    'Artificial Neural Network': Sequential([
        Dense(units=100, activation='relu', input_shape=(X_train_scaled.shape[1],)),
        Dropout(0.2),  # Add dropout with a dropout rate of 20%
        Dense(units=1, activation='sigmoid')
    ])
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    if isinstance(model, Sequential):  # For TensorFlow models
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, verbose=0)
        accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)[1]
    else:  # For scikit-learn models
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy

# Print the results
print("Accuracy Scores:")
for name, acc in results.items():
    print(f"{name}: {acc}")

"""# Step 10: Compare result
## Step 10.1: Accuracy Scores
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Define models and their accuracy scores (replace these with your actual results)
models = ['Logistic Regression', 'Support Vector Machine', 'Random Forest', 'K-Nearest Neighbors', 'Gaussian Naive Bayes', 'Decision Tree', 'Artificial Neural Network']
accuracy_scores = [0.9658, 0.9589, 1.0, 0.9343, 0.9193, 1.0, 0.9767]

# Create a bar plot for model comparison
plt.figure(figsize=(10, 6))
sns.barplot(x=accuracy_scores, y=models, palette='viridis')
plt.xlabel('Accuracy Score')
plt.title('Model Comparison: Accuracy Scores')
plt.show()

"""# Step 10.2: Correlation Heatmap"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate correlation coefficients between features and target
correlations = X.corrwith(y)

# Create a dataframe for visualization
corr_df = pd.DataFrame({'Features': correlations.index, 'Correlation': correlations.values})
corr_df = corr_df.set_index('Features')

# Plotting the correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

"""# Step 10.3:feature_importances"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# Initialize and train the Random Forest model
rf_model = RandomForestClassifier()
rf_model.fit(X_train_scaled, y_train)

# Get feature importances from the trained model
importances = rf_model.feature_importances_

# Get feature names
feature_names = X.columns

# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importance - Random Forest")
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.tight_layout()
plt.show()

"""# Step 10.4:Confusion Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Initialize models as a dictionary
models = {
    'Logistic Regression': LogisticRegression(),
    'Support Vector Machine': SVC(),
    'Random Forest': RandomForestClassifier(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Gaussian Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier(),
    'Artificial Neural Network': Sequential([
        Dense(units=100, activation='relu', input_shape=(X_train_scaled.shape[1],)),
        Dropout(0.2),  # Add dropout with a dropout rate of 20%
        Dense(units=1, activation='sigmoid')
    ])
}

# Train each model and calculate confusion matrix
for name, model in models.items():
    if name == 'Artificial Neural Network':
        # Compile the ANN model
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    if name == 'Artificial Neural Network':
        y_pred = (y_pred > 0.5)  # Convert probabilities to binary predictions

    cm = confusion_matrix(y_test, y_pred)

    # Plot confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

"""# Step 11: Metric Calculation"""

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score


# Train each model and calculate confusion matrix
for name, model in models.items():
    if name == 'Artificial Neural Network':
        # Compile the ANN model
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    if name == 'Artificial Neural Network':
        y_pred = (y_pred > 0.5)  # Convert probabilities to binary predictions

    cm = confusion_matrix(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Plot confusion matrix with additional metrics annotations
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['Predicted Negative', 'Predicted Positive'],
                yticklabels=['Actual Negative', 'Actual Positive'])
    plt.title(f'Confusion Matrix - {name}\nPrecision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

